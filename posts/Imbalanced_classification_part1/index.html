<!DOCTYPE html><html lang="en" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Imbalanced data classification part1 | Kwon_sun_cheol</title><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Imbalanced data classification part1" /><meta name="author" content="Kwon Suncheol" /><meta property="og:locale" content="en_US" /><meta name="description" content="Cute imbalanced image1 https://medium.com/@kr.vishwesh54/a-creative-way-to-deal-with-class-imbalance-without-generating-synthetic-samples-4cfad099d405 &#8617;" /><meta property="og:description" content="Cute imbalanced image1 https://medium.com/@kr.vishwesh54/a-creative-way-to-deal-with-class-imbalance-without-generating-synthetic-samples-4cfad099d405 &#8617;" /><link rel="canonical" href="https://classicmania.github.io/posts/Imbalanced_classification_part1/" /><meta property="og:url" content="https://classicmania.github.io/posts/Imbalanced_classification_part1/" /><meta property="og:site_name" content="Kwon_sun_cheol" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-01-09T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Imbalanced data classification part1" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Kwon Suncheol" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"headline":"Imbalanced data classification part1","description":"Cute imbalanced image1 https://medium.com/@kr.vishwesh54/a-creative-way-to-deal-with-class-imbalance-without-generating-synthetic-samples-4cfad099d405 &#8617;","datePublished":"2021-01-09T00:00:00+09:00","dateModified":"2021-01-09T00:00:00+09:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://classicmania.github.io/posts/Imbalanced_classification_part1/"},"url":"https://classicmania.github.io/posts/Imbalanced_classification_part1/","author":{"@type":"Person","name":"Kwon Suncheol"},"@context":"https://schema.org"}</script><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="preload" as="style" href="/assets/css/post.css"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /><link rel="preload" as="script" href="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" async></script> <script src="/assets/js/post.min.js" async></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); </script> <script src="/app.js" defer></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/sample/polymath.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/">Kwon_sun_cheol</a></div><div class="site-subtitle font-italic">Data lover</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <a href="https://github.com/classicmania" target="_blank"> <i class="fab fa-github-alt"></i> </a> <a href="https://classicmania33.medium.com/" target="_blank"> <i class="fab fa-medium"></i> </a> <a href="https://www.linkedin.com/in/%EC%88%9C%EC%B2%A0-%EA%B6%8C-9218a8180/" target="_blank"> <i class="fab fa-linkedin"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Imbalanced data classification part1</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Imbalanced data classification part1</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sat, Jan 9, 2021, 12:00 AM +0900" > Jan 9 <i class="unloaded">2021-01-09T00:00:00+09:00</i> </span> by <span class="author"> Kwon Suncheol </span></div><div> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Jan 24, 2021, 10:59 PM +0900" > Jan 24 <i class="unloaded">2021-01-24T22:59:59+09:00</i> </span></div></div><div class="post-content"><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/Imbalanced_image.jpg" alt="Imbalanced_image" /><em>Cute imbalanced image<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></em></p><p><br /></p><p>지도학습에서 분류 문제를 다룰 때 Imbalanced classification인 경우가 많았습니다. 예를 들어 이커머스 데이터를 활용하여 개별적인 고객의 이탈을 예측하는 모델을 만들 때 위의 문제를 발견할 수 있었습니다. <strong>실무에서 이탈 예측 태스크를 진행하면서 ‘불균형 데이터를 어떻게 다룰 것인가’에 대하여 생각한 것들을 두 개의 블로그 컨텐츠로 정리하는 시간을 가졌습니다.</strong> 해당 글은 그 중 첫번째 글입니다.</p><h2 id="imbalanced-data">Imbalanced data</h2><hr /><h3 id="definition-of-imbalanced-data">Definition of Imbalanced data</h3><p>처음에는 데이터의 반응변수의 분포가 고르게 퍼진 Unbalanced data와 다르게 <strong>Imbalanced data는 도메인 특성상 내재적인 이유로 처음부터 반응변수의 분포가 왜곡된 데이터로 정의</strong>할 수 있습니다. 이러한 문제는 앞에서 언급한 ‘고객 이탈 예측’, ‘이상치 탐지’, ‘스팸 탐지’까지 다양한 상황에서 직면할 수 있습니다.</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Case<th style="text-align: center">심각한 불균형 O<th style="text-align: center">심각한 불균형 X<th style="text-align: center">충분한 데이터 샘플 O<th style="text-align: center">충분한 데이터 샘플 X<tbody><tr><td style="text-align: center">case 1<td style="text-align: center"><span style="color:red">O</span><td style="text-align: center"><span style="color:blue">X</span><td style="text-align: center"><span style="color:red">O</span><td style="text-align: center"><span style="color:blue">X</span><tr><td style="text-align: center">case 2<td style="text-align: center"><span style="color:red">O</span><td style="text-align: center"><span style="color:blue">X</span><td style="text-align: center"><span style="color:blue">X</span><td style="text-align: center"><span style="color:red">O</span><tr><td style="text-align: center">case 3<td style="text-align: center"><span style="color:blue">X</span><td style="text-align: center"><span style="color:red">O</span><td style="text-align: center"><span style="color:red">O</span><td style="text-align: center"><span style="color:blue">X</span><tr><td style="text-align: center">case 4<td style="text-align: center"><span style="color:blue">X</span><td style="text-align: center"><span style="color:red">O</span><td style="text-align: center"><span style="color:blue">X</span><td style="text-align: center"><span style="color:red">X</span></table></div><p>각각의 케이스마다 접근 방법을 다르게 할 필요성이 있지만 특히 case 2번[심각한 불균형이면서 충분한 데이터 샘플을 확보하지 못한 경우]같은 경우는 분류 모델을 구성하기 상당히 힘듭니다. 위의 테이블에서 우리는 데이터 자체의 크기, 노이즈(Noise)를 유발하는 데이터, 그리고 왜도가 높은 데이터 분포가 특정한 분류 문제를 어렵게 만듬을 알 수 있습니다.</p><p><br /></p><h2 id="evaluation-metrics-of-imbalanced-data-classification">Evaluation Metrics of Imbalanced data classification</h2><hr /><h3 id="three-families-of-evaluation-metrics-of-classification">Three families of evaluation metrics of classification</h3><p>다음의 논문<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>에서 분류 문제에 대한 평가 지표를 다음과 같이 세 가지로 구분하였습니다.</p><ul><li>Threshold Metrics<li>Ranking Metrics<li>Probability Metrics</ul><p><br /></p><p>세 가지 분류 지표군들에서 Train data와 Test data간의 분포가 비슷하다고 가정할 때 사용되는 ‘Threshold Metrics’를 알아보겠습니다.</p><h3 id="threshold-metrics">Threshold Metrics</h3><p>‘Threshold Metrics’는 분류 예측 모델의 오분류를 정량화한 지표입니다. 다음과 같은 표를 많이 보셨을 겁니다.</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center"> <th style="text-align: center">Positive Prediction<th style="text-align: center">Negative Prediction<tbody><tr><td style="text-align: center"><strong>Positive Class</strong><td style="text-align: center">TP(True Positive)<td style="text-align: center">FN(False Negative)<tr><td style="text-align: center"><strong>Negative Class</strong><td style="text-align: center">FP(False Positive)<td style="text-align: center">TN(True Negative)</table></div><p>이커머스에서 이탈 예측 문제를 해당 표와 연결하면 다음과 같습니다.</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center"> <th style="text-align: center">이탈로 예측<th style="text-align: center">이탈이 아니라고 예측<tbody><tr><td style="text-align: center"><strong>실제 이탈 O</strong><td style="text-align: center">TP(True Positive)<td style="text-align: center">FN(False Negative)<tr><td style="text-align: center"><strong>실제 이탈 X</strong><td style="text-align: center">FP(False Positive)<td style="text-align: center">TN(True Negative)</table></div><p>사기 탐지(Fraud detection) 영역에서는 다음과 같은 그림으로 위의 표의 케이스를 응용하여 해석할 수 있습니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/false-positive-test-diagram.png" alt="false-positive-test-diagram" /><em>false-positive test diagram in fraud detection<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></em></p><p><br /></p>\[Accuracy = \frac {TP + TN}{TP+FN+FP+TN}\]<p><br /></p>\[Error = 1 - Accuracy = \frac {FP + FN}{TP+FN+FP+TN}\]<p><br /></p><h3 id="sensitivity-specificity-metrics">Sensitivity-Specificity Metrics</h3><p>일반적인 분류 문제에서는 정확도를 많이 참고합니다. 하지만 사기 탐지 데이터와 같이 극단적으로 불균형된 데이터인 경우 정확도는 크게 의미가 없습니다. FN(False Negative)와 FP(False Postivie) 모두 예측 모델이 틀린 경우들이지만 사기 탐지인 경우 FN일 때가 FP인 경우보다 훨씬 치명적입니다. <br /> 이것을 한번 위의 표와 엮으면 다음과 같은 지표를 고안하여 해석할 수 있습니다.</p><p><br /></p>\[Sensitivity = \frac {TP}{TP+FN}\]<p><br /></p>\[Specificity = \frac {TN}{FP+TN}\]<p><br /></p><p>사기 탐지 같은 경우 민감도를 특이도보다 더 중요한 지표로 삼을 것입니다. 이 해석을 제가 마주한 문제에 그대로 비유하면 이탈을 하지 않는데 이탈을 하는 것으로 예측하는 것이 미래 시점에 이탈이 발생했는데 잘못 예측하는 것보다 덜 치명적일 것입니다. 하지만 일반적으로 이탈하는 고객이 이탈하지 않는 고객보다 훨씬 많습니다. 그러므로 기계적인 해석이 아니라 해당 이커머스 업체의 특성에 따라서 특이도와 민감도의 차이를 비교해야 합니다.</p><p><br /></p><p>만약 특이도와 민감도 두 가지 모두를 고려하고 싶을 때는 두 지표의 기하평균인 G-mean을 사용합니다.</p><p><br /></p>\[G-mean = \sqrt{Sensitiviy \times Specificity}\]<p><br /></p><h3 id="roc-curve">ROC curve</h3><p>ROC curve는 양성 클래스의 이진 분류 모델의 성능을 요약해주는 그래프입니다. X축은 FPR(False Positive Rate)이고 Y축은 TPR(True Positive Rate)입니다. 먼저 TPR과 FPR을 살펴 보겠습니다.</p><p><br /></p>\[TPR = \frac {TP}{TP+FN}\]<p><br /></p>\[FPR = \frac {FP}{FP+TN}\]<p><br /></p><p>TPR은 양성 클래스를 얼마나 잘 예측한지를 알려줍니다.FPR은 전체 음성 클래스에서 양성으로 잘못 예측할 비율을 의미합니다. TPR을 FPR로 나눴을 경우 다음과 같습니다.</p><p><br /></p>\[\frac{(\frac {TP}{TP+FN})}{(\frac {FP}{FP+TN})} = \frac{TP \times (FP+TN)}{ FP \times (TP+FN)}\]<p><br /></p><p>양성 클래스(TP+FN)와 음성 클래스(FP+TN)는 이미 상수이므로 ROC값을 변화시키는 것은 TP와 FP입니다. 하지만 TP와 FP는 Threshold에 따라 달라집니다. 그러므로 실제 클래스 및 모델 예측값의 분포 관계에 따라서 ROC AUC를 다음과 같이 그릴 수 있을 것입니다.</p><p><br /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/ROC_curve_best1.png" alt="ROC_curve_best1" /><em>Best ROC curve dist<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></em> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/ROC_curve_best2.png" alt="ROC_curve_best2" /><em>Best ROC AUC</em></p><p><br /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/ROC_curve_mid1.png" alt="ROC_curve_mid1" /><em>General curve dist</em> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/ROC_curve_mid2.png" alt="ROC_curve_mid2" /><em>General ROC AUC</em></p><p><br /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/ROC_curve_501.png" alt="ROC_curve_501" /><em>ROC half curve dist</em> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/ROC_curve_502.png" alt="ROC_curve_502" /><em>half ROC UC</em></p><p><br /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/ROC_curve_worst1.png" alt="ROC_curve_worst1" /><em>ROC worst curve dist</em> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/ROC_curve_worst2.png" alt="ROC_curve_worst2" /><em>Worst ROCAUC</em></p><p><br /></p><h3 id="precision-recall-metrics">Precision-Recall Metrics</h3><p>수식상으로 Precision과 Recall을 살펴보면 다음과 같습니다.</p><p><br /></p>\[Precision = \frac {TP}{TP+FP}\]<p><br /></p>\[Recall = \frac {TP}{TP+FN}\]<p><br /></p><p>수식의 의미를 사기 탐지 태스크와 이탈 예측 태스크에 각각 맞추어서 살펴보겠습니다. 사기 탐지 태스크에서 정밀도의 의미는 해당 클래스가 ‘사기’로 예측하였을 때 실제로 사기일 비율을 의미합니다. 이와 유사하게 <strong>이탈 예측 태스크에서 정밀도의 의미는 해당 클래스가 ‘이탈’로 예측하였을 때 실제로 ‘이탈’을 할 비율</strong>을 뜻합니다.<br /> <br /> 재현율은 수식상으로 민감도와 같습니다. 사기 탐지 태스크에서 재현율은 해당 클래스가 실제로 ‘사기’일 때 사기로 예측할 확률을 말합니다. <strong>이탈 예측 태스크에서는 실제로 ‘이탈’일 때 이탈로 예측할 비율</strong>을 의미합니다.</p><p><br /></p><p>수식에서 알 수 있듯이 정밀도와 재현율의 관계는 다음과 같습니다.</p>\[\frac{(\frac {TP}{TP+FP})}{(\frac {TP}{TP+FN})} = \frac{TP \times (TP+FN)}{ TP \times (TP+FP)} = \frac{TP+FN}{TP+FP}\]<p>ROC curve와 마찬가지로 Threshold를 설정함에 따라서 값이 달라집니다. 하지만 음성 클래스와 양성 클래스를 모두 고려하는 ROC curve와 다르게 PR curve는 소수 클래스인 양성 클래스만을 고려합니다. 이 점을 고려할 때 실무적으로 굉장히 Skewed한 클래스 분포를 가지면서 ROC AUC가 너무 높은 값을 가졌을 때 소수 클래스에 초점을 맞춘 PR AUC를 확인하여 분류 모델의 성능을 종합적으로 확인할 수 있습니다.</p><p><br /></p><p>Recall-Precision을 그래프로 그리고 다음과 같은 경우가 있을 것입니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/Precision_Recall_Curve_image1.png" alt="Precision_Recall_Curve_image1" /><em>General Precision-Recall Curve<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></em></p><p>모델의 성능이 아주 뛰어나거나 안좋으면 다음과 같이 그려질 것입니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/Precision_Recall_Curve_image2.png" alt="Precision_Recall_Curve_image2" /><em>Best Precision-Recall Curve</em></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/Precision_Recall_Curve_image3.png" alt="Precision_Recall_Curve_image3" /><em>Worst Precision-Recall Curve</em></p><p><br /></p><h2 id="data-sampling">Data sampling</h2><hr /><ul><li><strong>데이터 샘플링은 반응 변수의 분포를 유지시키기 위한 방법입니다.</strong> 예측 모델을 신중히 선택하는 만큼 다양한 데이터 샘플링 기법들 중 가장 자신의 문제 상황에 맞는 방법을 결정해야 합니다.<li>만약 데이터가 왜곡되어 있다면 많은 머신러닝 모델들은 다수를 차지하는 클래스에서 관측되는 예측 변수들의 가능도에 초점이 맞추어 질 것입니다.<li>이런 경우, 이탈 예측과 같이 소수 클래스와 예측 변수들의 관계를 포착하는 것이 중요한 태스크는 예측 모델을 구성할 때 심각한 문제에 직면할 수 있습니다.</ul><h3 id="random-sampling">Random sampling</h3><h4 id="random-undersampling">Random Undersampling</h4><ul><li>학습 데이터 세트에서 다수 클래스를 랜덤하게 선택한 후 지우는 샘플링 기법입니다.<li>불균형한 데이터이지만 소수를 차지하는 클래스가 충분한 관측치를 가지고 있는 경우 사용합니다.<li>다만 Undersampling 기법의 특성상 다수의 클래스에서 지워지는 데이터가 분류 경계를 짓는데 아주 중요할 경우 예측 모델 정확도가 떨어질 수 있습니다.</ul><p><br /></p><p>RUS에 대한 이미지를 살펴보면 다음과 같습니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/random_undersampling_image.png" alt="random_undersampling_image" /><em>RUS</em><sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup></p><h4 id="random-oversampling">Random Oversampling</h4><ul><li>소수 클래스의 데이터 중에서 일정한 수를 랜덤하게 선택하여 다수 클래스의 수만큼 복제하는 기법입니다.<li>왜곡된 분포에 크게 영향을 받는 머신러닝 모델에 적합하지만 소수의 클래스에 과적합이 발생하여 일반화에 악영향을 끼칠 수 있습니다.<li>만약 다수 클래스의 데이터가 엄청 클 경우 Oversampling된 후의 데이터를 학습시킬 때 계산 속도가 오래 걸립니다.<li>이런 악영향을 방지하기 위해서 Raw data에 예측 모델과 Random Oversapling을 적용한 데이터에 적합시킨 예측 모델간의 성능을 비교할 필요가 있습니다.</ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/Oversampling_image.jpeg" alt="random_oversampling_image" /><em width="200" height="500">ROS</em><sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup></p><p><br /></p><h3 id="undersampling">Undersampling</h3><ul><li>단순히 다수 클래스의 샘플 수를 랜덤하게 뽑아서 제거하는 것이 아닌 일정한 논리에 따라 데이터의 Feature Space를 최대한 유지시키는 방향으로 UnderSampling을 진행할 수 있습니다.<li>먼저 다음의 상황을 가정해 봅니다.<ul><li>아래의 표는 개별적인 특성이 담긴 테이블입니다.<li>다음과 같은 기준으로 사람들을 묶을 수 있을 것입니다.<ul><li><strong>비슷한 특질이 있는 사람들 끼리 묶기!</strong><li><strong>비슷하지 않는 특색을 가진 사람들을 구분하기!</strong><li><strong>첫번째 방법과 두번째 방법을 섞어서 묶기!</strong></ul></ul></ul><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Feature<th style="text-align: center"><span style="color:red">Lee</span><th style="text-align: center"><span style="color:red">Koo</span><th style="text-align: center"><span style="color:blue">Kwon</span><th style="text-align: center"><span style="color:blue">Kim</span><th style="text-align: center"><span style="color:blue">Ko</span><th style="text-align: center"><span style="color:black">Cho</span><th style="text-align: center">Oh<tbody><tr><td style="text-align: center"><strong>Feature_a</strong><td style="text-align: center">O<td style="text-align: center">O<td style="text-align: center">O<td style="text-align: center">O<td style="text-align: center">O<td style="text-align: center">X<td style="text-align: center">X<tr><td style="text-align: center"><strong>Feature_b</strong><td style="text-align: center">O<td style="text-align: center">O<td style="text-align: center">X<td style="text-align: center">X<td style="text-align: center">X<td style="text-align: center">O<td style="text-align: center">O<tr><td style="text-align: center"><strong>Feature_c</strong><td style="text-align: center">X<td style="text-align: center">X<td style="text-align: center">O<td style="text-align: center">O<td style="text-align: center">O<td style="text-align: center">X<td style="text-align: center">X</table></div><p><br /></p><p>Undersampling도 앞의 세 가지 경우와 비슷합니다. 구체적으로 다음과 같이 기법을 세분화할 수 있습니다. 세부적인 Sampling 기법의 방법들에 대해 알고 싶으시면 아래 주석의 논문들을 참고하시길 바랍니다.</p><ul><li>유지할 데이터를 선택하는 방법<ul><li>Near Miss Undersampling<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup><ul><li>NearMiss-1 : 소수 클래스로부터 다수 클래스의 샘플들 중 가장 평균 거리가 작은 k개의 샘플을 뽑습니다.<li>NearMiss-2 : 소수 클래스로부터 다수 클래스의 샘플들 중 가장 평균 거리가 먼 k개의 샘플을 뽑습니다.<li>NearMiss-3 : 소수의 클래스의 개별 샘플로부터 가장 거리가 가까운 샘플들을 뽑습니다.</ul><li>Condensed Nearest Neighbor Rule<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup><ul><li>CNN기법은 모델의 성능에 해를 끼치지 않는 데이터의 부분집합을 찾는 방법입니다.<li>자세한 내용은 아래의 코드와 함께 설명하였습니다.</ul></ul><li>제거할 데이터를 선택하는 방법<ul><li>Tomek Links<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup><li>ENN(Edited Nearest Neighbor Rule)<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup></ul><li>첫번째와 두번째를 적절히 섞은 방법<ul><li>One-Sided Selection<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup><ul><li>Tomek Links -&gt; CNN</ul><li>Neighborhood Cleansing Rule<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">13</a></sup><ul><li>CNN -&gt; ENN</ul></ul></ul><p><br /></p><h4 id="유지할-데이터를-선택하는-방법">유지할 데이터를 선택하는 방법</h4><p>가상의 불균형 데이터를 만들어서 Under sampling의 효과를 살펴보겠습니다. 먼저 10000개 샘플에 관하여 두 가지 예측 변수들과 클래스가 0이거나 1인 클래스를 구성하겠습니다. 노이즈 데이터의 비율은 실험의 명확성을 위해 0으로 설정하였고 다수 클래스에 대한 가중치는 90%로 설정하였습니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mc</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
		 <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.90</span><span class="p">],</span>
                <span class="n">n_features</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">random_state</span> <span class="o">=</span> <span class="mi">33</span><span class="p">,</span>
                <span class="n">flip_y</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">Counter</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>Counter({0: 9001, 1: 999})
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/raw_data_scatter_plot.png" alt="raw_data_scatter_plot" /><em>Raw data Scatter Plot</em></p><p><br /></p><h5 id="nearmiss-3">Nearmiss-3</h5><p>다수 클래스와 소수 클래스의 Boundary를 알 수 있게 해주는 “NearMiss-3” 기법[각각의 소수 클래스로부터 거리가 가까운 다수 클래스내 샘플 수 기준은 5입니다]을 적용한 데이터의 Scatter plot을 살펴보면 다음과 같습니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/nearmiss_plot.png" alt="nearmiss_plot" /><em>Nearmiss3 data Scatter Plot</em></p><p><br /></p><h5 id="condensed-nearest-neighbour">Condensed nearest neighbour</h5><p>다음은 imbalanced-learn library에서 Condensed nearest neighbour<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">14</a></sup>를 구현한 코드입니다. 코드 내용을 통해 CNN은 다음과 같이 동작함을 알 수 있습니다.</p><ul><li>Random Seed를 설정한 후 다수 클래스에서 하나의 샘플을 임의적으로 추출합니다.<li>모든 소수 클래스의 샘플들과 앞에서 추출한 샘플을 사용하여 하나의 부분집합(C)을 생성합니다.<li>모든 다수 클래스를 이용하여 부분집합 S를 만듭니다.<li>Set C를 KNN에 적합시킨 후 모델이 제대로 분류한 샘플들은 고르지 않고 제대로 분류되지 않은 샘플들을 C에 Append시킵니다.<li>해당 알고리즘은 이중 for문을 쓰는 구조이므로 knn의 하이퍼 파라미터를 작게 해주는 것이 일반적으로 좋습니다.</ul><p><br /></p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">_fit_resample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_validate_estimator</span><span class="p">()</span>

    <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">target_stats</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">class_minority</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">target_stats</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">target_stats</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>
    <span class="n">idx_under</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">target_class</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">target_class</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">sampling_strategy_</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="c1"># Randomly get one sample from the majority class
</span>            <span class="c1"># Generate the index to select
</span>            <span class="n">idx_maj</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">target_class</span><span class="p">)</span>
            <span class="n">idx_maj_sample</span> <span class="o">=</span> <span class="n">idx_maj</span><span class="p">[</span>
                <span class="n">random_state</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span>
                    <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">high</span><span class="o">=</span><span class="n">target_stats</span><span class="p">[</span><span class="n">target_class</span><span class="p">],</span>
                    <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">n_seeds_S</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">]</span>

            <span class="c1"># Create the set C - One majority samples and all minority
</span>            <span class="n">C_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">np</span><span class="p">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">class_minority</span><span class="p">),</span> <span class="n">idx_maj_sample</span>
            <span class="p">)</span>
            <span class="n">C_x</span> <span class="o">=</span> <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C_indices</span><span class="p">)</span>
            <span class="n">C_y</span> <span class="o">=</span> <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">C_indices</span><span class="p">)</span>

            <span class="c1"># Create the set S - all majority samples
</span>            <span class="n">S_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">target_class</span><span class="p">)</span>
            <span class="n">S_x</span> <span class="o">=</span> <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">S_indices</span><span class="p">)</span>
            <span class="n">S_y</span> <span class="o">=</span> <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">S_indices</span><span class="p">)</span>

            <span class="c1"># fit knn on C
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">estimator_</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">C_x</span><span class="p">,</span> <span class="n">C_y</span><span class="p">)</span>

            <span class="n">good_classif_label</span> <span class="o">=</span> <span class="n">idx_maj_sample</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="c1"># Check each sample in S if we keep it or drop it
</span>            <span class="k">for</span> <span class="n">idx_sam</span><span class="p">,</span> <span class="p">(</span><span class="n">x_sam</span><span class="p">,</span> <span class="n">y_sam</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">S_x</span><span class="p">,</span> <span class="n">S_y</span><span class="p">)):</span>

                <span class="c1"># Do not select sample which are already well classified
</span>                <span class="k">if</span> <span class="n">idx_sam</span> <span class="ow">in</span> <span class="n">good_classif_label</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="c1"># Classify on S
</span>                <span class="k">if</span> <span class="ow">not</span> <span class="n">issparse</span><span class="p">(</span><span class="n">x_sam</span><span class="p">):</span>
                    <span class="n">x_sam</span> <span class="o">=</span> <span class="n">x_sam</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">pred_y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">estimator_</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_sam</span><span class="p">)</span>

                <span class="c1"># If the prediction do not agree with the true label
</span>                <span class="c1"># append it in C_x
</span>                <span class="k">if</span> <span class="n">y_sam</span> <span class="o">!=</span> <span class="n">pred_y</span><span class="p">:</span>
                    <span class="c1"># Keep the index for later
</span>                    <span class="n">idx_maj_sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">idx_maj_sample</span><span class="p">,</span> <span class="n">idx_maj</span><span class="p">[</span><span class="n">idx_sam</span><span class="p">]</span>
                    <span class="p">)</span>

                    <span class="c1"># Update C
</span>                    <span class="n">C_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">C_indices</span><span class="p">,</span> <span class="n">idx_maj</span><span class="p">[</span><span class="n">idx_sam</span><span class="p">])</span>
                    <span class="n">C_x</span> <span class="o">=</span> <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C_indices</span><span class="p">)</span>
                    <span class="n">C_y</span> <span class="o">=</span> <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">C_indices</span><span class="p">)</span>

                    <span class="c1"># fit a knn on C
</span>                    <span class="bp">self</span><span class="p">.</span><span class="n">estimator_</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">C_x</span><span class="p">,</span> <span class="n">C_y</span><span class="p">)</span>

                    <span class="c1"># This experimental to speed up the search
</span>                    <span class="c1"># Classify all the element in S and avoid to test the
</span>                    <span class="c1"># well classified elements
</span>                    <span class="n">pred_S_y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">estimator_</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">S_x</span><span class="p">)</span>
                    <span class="n">good_classif_label</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span>
                        <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                            <span class="n">idx_maj_sample</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">pred_S_y</span> <span class="o">==</span> <span class="n">S_y</span><span class="p">)</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

            <span class="n">idx_under</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">idx_under</span><span class="p">,</span> <span class="n">idx_maj_sample</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">idx_under</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span>
                <span class="p">(</span><span class="n">idx_under</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">target_class</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">sample_indices_</span> <span class="o">=</span> <span class="n">idx_under</span>

    <span class="k">return</span> <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">idx_under</span><span class="p">),</span> <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">idx_under</span><span class="p">)</span>
</pre></table></code></div></div><p>앞의 불균형 데이터에 CNN[k value in KNN = 5]을 적용하면 다음과 같습니다.</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>Raw data --&gt; Counter({0: 9001, 1: 999})
After CNN processing --&gt; Counter({0: 153, 1: 999})
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/Condensed_Nearest_Neighbour_image.png" alt="CNN" /><em>After CNN processing data Scatter Plot</em></p><p>CNN은 Under sampling 기법 중 ‘유지할 데이터를 선택하는 방법’에 충실하지만 다음과 같은 코드에서 임의성을 발견할 수 있습니다.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="k">for</span> <span class="n">target_class</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">target_class</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">sampling_strategy_</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="c1"># Randomly get one sample from the majority class
</span>            <span class="c1"># Generate the index to select
</span>            <span class="n">idx_maj</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">target_class</span><span class="p">)</span>
            <span class="n">idx_maj_sample</span> <span class="o">=</span> <span class="n">idx_maj</span><span class="p">[</span>
                <span class="n">random_state</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span>
                    <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">high</span><span class="o">=</span><span class="n">target_stats</span><span class="p">[</span><span class="n">target_class</span><span class="p">],</span>
                    <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">n_seeds_S</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">]</span>
</pre></table></code></div></div><h4 id="제거할-데이터를-선택하는-방법">제거할 데이터를 선택하는 방법</h4><h5 id="tomek-links">Tomek Links</h5><p>이런 초반의 임의성은 유지하지 않아도 되는 데이터를 부분집합에 속하게 만듭니다. 이것을 제어하기 위해서 엄밀한 쌍을 하나 만듭니다. 그 쌍은 다음과 같이 정의됩니다</p><blockquote><p>Tomek Link : 인스턴스 a와 인스턴스 b는 다음과 같을 때 ‘Tomek Link’라 한다. 인스턴스의 가장 가까운 이웃은 인스턴스 b이다. 마찬가지로 인스턴스 b와 가장 가까운 이웃은 인스턴스 a이다. 이 때 인스턴스 a와 인스턴스 b는 서로 다른 클래스에 속해야 한다.</p></blockquote><p>이렇게 되면 앞의 CNN의 임의성을 제거되면서 각 클래스간의 경계를 명확히 알 수 있습니다. TomekLink(k = 2)의 결과를 확인해 보겠습니다.</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>Counter({0: 9001, 1: 999})
Counter({0: 8953, 1: 999})
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/Tomeklink.png" alt="Tomeklink" /><em>After TomekLink processing data Scatter Plot</em></p><p>위의 그래프를 통해 알 수 있듯이 각 클래스의 경계를 아는 것만으로는 Undersampling의 큰 효과를 발휘할 수 없습니다. 그래서 실무상에서는 Tomek Link를 통해 Noise와 경계값을 제거한 후 다른 Under Sampling(Ex : CNN) 기법을 같이 사용합니다. 두 가지 기법(Tomek Link + CNN)을 같이 사용한 결과는 다음과 같습니다.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/Tomek_and_CNN.png" alt="Tomek_and_CNN" /><em>After TomekLink &amp; CNN processing data Scatter Plot</em></p><p><br /></p><h3 id="oversampling">Oversampling</h3><ul><li>Oversampling 기법은 불균형 데이터의 분포를 처리할 때 소수 클래스를 다수 클래스의 샘플 수에 맞추어 늘리는 방법입니다.<li>제일 간단한 방법은 소수 클래스의 샘플을 단순하게 복제하는 것입니다. 다만 이럴 경우 학습하려는 모델에 어떤 새로운 정보를 제공할 수 없습니다.<li>대신에 소수 클래스에 대하여 새로운 정보를 주면서 augmentation하는 기법 중 SMOTE(Synthetic Minority Oversampling Technique)<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">15</a></sup>가 있습니다.</ul><h4 id="smote">SMOTE</h4><p>다음은 imbalanced-learn library에서 SMOTE를 구현한 코드입니다. 위의 논문 및 Borderline 관련 추가 논문<sup id="fnref:16" role="doc-noteref"><a href="#fn:16" class="footnote" rel="footnote">16</a></sup>과 코드를 통해 SMOTE는 다음과 같이 동작함을 알 수 있습니다.</p><ul><li>먼저 소수 클래스의 샘플들을 랜덤으로 선택합니다.<li>선택된 임의의 개별적인 샘플[S1]에서 Featrue Space에서 거리가 가장 가까우면서 소수 클래스에 속한 k개의 샘플들[S2]을 선택합니다.<li>S1과 S2를 직선으로 연결한 후 그 직선상에서 새로운 데이터를 생성합니다.<ul><li>일반적으로 직선상에 생성된 샘플은 직선과 거리가 먼 샘플보다 오분류가 될 가능성이 더 높습니다.<li>다시 말해서 거리가 먼 샘플들은 KNN 알고리즘에 따라소수 클래스보다 다수 클래스에 속할 가능성이 높을 때 오분류의 가능성이 높은 Danger 샘플로 간주됩니다.<ul><li>borderline-SMOTE1 : Danger 샘플과 소수 클래스 간의 거리차이의 가중치를 0~1로 줍니다.<li>borderline-SMOTE2 : Danger 샘플과 소수 클래스 간의 거리차이의 가중치를 0~0.5로 줍니다. 그래서 ‘borderline-SMOTE1’보다 더 가까운 샘플들을 생성합니다.</ul><li>신기하게도 SMOTE 알고리즘의 객체를 소수 클래스가 아닌 다수 클래스에 적용을 해도 같은 현상이 발견될 수 있습니다.</ul></ul><p><br /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post_img/SMOTE_image.png" alt="SmoteImage" /><em>SMOTE Image</em></p><p><br /></p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">_validate_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">_validate_estimator</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">nn_m_</span> <span class="o">=</span> <span class="n">check_neighbors_object</span><span class="p">(</span>
        <span class="s">"m_neighbors"</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">m_neighbors</span><span class="p">,</span> <span class="n">additional_neighbor</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">nn_m_</span><span class="p">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="s">"n_jobs"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_jobs</span><span class="p">})</span>
    <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">kind</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s">"borderline-1"</span><span class="p">,</span> <span class="s">"borderline-2"</span><span class="p">):</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span>
            <span class="s">'The possible "kind" of algorithm are '</span>
            <span class="s">'"borderline-1" and "borderline-2".'</span>
            <span class="s">"Got {} instead."</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">kind</span><span class="p">)</span>
        <span class="p">)</span>

<span class="k">def</span> <span class="nf">_fit_resample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">_validate_estimator</span><span class="p">()</span>

    <span class="n">X_resampled</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">y_resampled</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">class_sample</span><span class="p">,</span> <span class="n">n_samples</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">sampling_strategy_</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">n_samples</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">target_class_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">class_sample</span><span class="p">)</span>
        <span class="n">X_class</span> <span class="o">=</span> <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">target_class_indices</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">nn_m_</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">danger_index</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_in_danger_noise</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">nn_m_</span><span class="p">,</span> <span class="n">X_class</span><span class="p">,</span> <span class="n">class_sample</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">"danger"</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">danger_index</span><span class="p">):</span>
            <span class="k">continue</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">nn_k_</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_class</span><span class="p">)</span>
        <span class="n">nns</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">nn_k_</span><span class="p">.</span><span class="n">kneighbors</span><span class="p">(</span>
            <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X_class</span><span class="p">,</span> <span class="n">danger_index</span><span class="p">),</span> <span class="n">return_distance</span><span class="o">=</span><span class="bp">False</span>
        <span class="p">)[:,</span> <span class="mi">1</span><span class="p">:]</span>

        <span class="c1"># divergence between borderline-1 and borderline-2
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">kind</span> <span class="o">==</span> <span class="s">"borderline-1"</span><span class="p">:</span>
            <span class="c1"># Create synthetic samples for borderline points.
</span>            <span class="n">X_new</span><span class="p">,</span> <span class="n">y_new</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_make_samples</span><span class="p">(</span>
                <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X_class</span><span class="p">,</span> <span class="n">danger_index</span><span class="p">),</span>
                <span class="n">y</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">class_sample</span><span class="p">,</span>
                <span class="n">X_class</span><span class="p">,</span>
                <span class="n">nns</span><span class="p">,</span>
                <span class="n">n_samples</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">sparse</span><span class="p">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X_new</span><span class="p">):</span>
                <span class="n">X_resampled</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X_resampled</span><span class="p">,</span> <span class="n">X_new</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">X_resampled</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X_resampled</span><span class="p">,</span> <span class="n">X_new</span><span class="p">))</span>
            <span class="n">y_resampled</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">y_resampled</span><span class="p">,</span> <span class="n">y_new</span><span class="p">))</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">kind</span> <span class="o">==</span> <span class="s">"borderline-2"</span><span class="p">:</span>
            <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">random_state</span><span class="p">)</span>
            <span class="n">fractions</span> <span class="o">=</span> <span class="n">random_state</span><span class="p">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

            <span class="c1"># only minority
</span>            <span class="n">X_new_1</span><span class="p">,</span> <span class="n">y_new_1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_make_samples</span><span class="p">(</span>
                <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X_class</span><span class="p">,</span> <span class="n">danger_index</span><span class="p">),</span>
                <span class="n">y</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">class_sample</span><span class="p">,</span>
                <span class="n">X_class</span><span class="p">,</span>
                <span class="n">nns</span><span class="p">,</span>
                <span class="nb">int</span><span class="p">(</span><span class="n">fractions</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span>
                <span class="n">step_size</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># we use a one-vs-rest policy to handle the multiclass in which
</span>            <span class="c1"># new samples will be created considering not only the majority
</span>            <span class="c1"># class but all over classes.
</span>            <span class="n">X_new_2</span><span class="p">,</span> <span class="n">y_new_2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_make_samples</span><span class="p">(</span>
                <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X_class</span><span class="p">,</span> <span class="n">danger_index</span><span class="p">),</span>
                <span class="n">y</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">class_sample</span><span class="p">,</span>
                <span class="n">_safe_indexing</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">y</span> <span class="o">!=</span> <span class="n">class_sample</span><span class="p">)),</span>
                <span class="n">nns</span><span class="p">,</span>
                <span class="nb">int</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">fractions</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">),</span>
                <span class="n">step_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">sparse</span><span class="p">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X_resampled</span><span class="p">):</span>
                <span class="n">X_resampled</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="n">vstack</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">X_resampled</span><span class="p">,</span> <span class="n">X_new_1</span><span class="p">,</span> <span class="n">X_new_2</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">X_resampled</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X_resampled</span><span class="p">,</span> <span class="n">X_new_1</span><span class="p">,</span> <span class="n">X_new_2</span><span class="p">))</span>
            <span class="n">y_resampled</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">y_resampled</span><span class="p">,</span> <span class="n">y_new_1</span><span class="p">,</span> <span class="n">y_new_2</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">X_resampled</span><span class="p">,</span> <span class="n">y_resampled</span>
</pre></table></code></div></div><ul><li>위와 같은 순서로 말미암아 다음과 같이 SMOTE의 장단점을 찾을 수 있습니다.<ul><li>새로운 정보를 제공해서 학습시키려는 모델이 결정 경계를 효율적으로 배울 수 있도록 합니다.<li>단순히 Undersampling 기법만 적용한는 것보다 SMOTE와 Under Sampling을 같이 활용할 때 모델 성능이 올라갑니다.<li>음성 클래스와 양성 클래스의 예측값과 실제값의 분포가 많이 겹칠 경우 SMOTE를 통한 오버샘플링된 샘플들이 오히려 결정 경계를 모호하게 만들 수 있습니다.</ul></ul><h4 id="adasyn">ADASYN</h4><p><br /></p><h3 id="mixed-sampling">Mixed Sampling</h3><ul><li>SMOTE + Tomek Links<li>SMOTE + Edited Nearest Neighbor Rule</ul><p><br /></p><h2 id="reference">Reference</h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>https://medium.com/@kr.vishwesh54/a-creative-way-to-deal-with-class-imbalance-without-generating-synthetic-samples-4cfad099d405 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:2" role="doc-endnote"><p>https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:3" role="doc-endnote"><p>https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:4" role="doc-endnote"><p>https://www.appsflyer.com/blog/click-flooding-detection-false-positive-challenge/ <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:5" role="doc-endnote"><p>https://towardsdatascience.com/gaining-an-intuitive-understanding-of-precision-and-recall-3b9df37804a7 <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:6" role="doc-endnote"><p>https://www.researchgate.net/figure/llustration-of-random-undersampling-technique_fig3_343326638 <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:7" role="doc-endnote"><p>https://medium.com/@patiladitya81295/dealing-with-imbalance-data-1bacc7d68dff10302/24590&amp;hl=ko&amp;sa=X&amp;ei=xSoBYJS3GpCbywTT47Uw&amp;scisig=AAGBfm0zNdcfXdPynWxoQ3FsFum2KdF9ow&amp;nossl=1&amp;oi=scholarr <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:8" role="doc-endnote"><p>https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:9" role="doc-endnote"><p>https://ieeexplore.ieee.org/document/1054155 <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:10" role="doc-endnote"><p>https://ieeexplore.ieee.org/document/4309452 <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:11" role="doc-endnote"><p>https://ieeexplore.ieee.org/document/4309137 <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:12" role="doc-endnote"><p>https://sci2s.ugr.es/keel/pdf/algorithm/congreso/kubat97addressing.pdf <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:13" role="doc-endnote"><p>https://link.springer.com/chapter/10.1007/3-540-48229-6_9 <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:14" role="doc-endnote"><p>https://github.com/scikit-learn-contrib/imbalanced-learn/blob/master/imblearn/under_sampling/_prototype_selection/_condensed_nearest_neighbour.py <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:15" role="doc-endnote"><p>http://scholar.google.co.kr/scholar_url?url=https://www.jair.org/index.php/jair/article/download/ <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:16" role="doc-endnote"><p>http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.9315&amp;rep=rep1&amp;type=pdf <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/ml/'>ML</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/imbalanced_data/" class="post-tag no-text-decoration" >Imbalanced_data</a> <a href="/tags/churn_prediction/" class="post-tag no-text-decoration" >Churn_prediction</a> <a href="/tags/classification/" class="post-tag no-text-decoration" >Classification</a> <a href="/tags/data_sampling/" class="post-tag no-text-decoration" >Data_sampling</a> <a href="/tags/evaluation_metric/" class="post-tag no-text-decoration" >Evaluation_metric</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Imbalanced data classification part1 - Kwon_sun_cheol&url=https://classicmania.github.io/posts/Imbalanced_classification_part1/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Imbalanced data classification part1 - Kwon_sun_cheol&u=https://classicmania.github.io/posts/Imbalanced_classification_part1/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Imbalanced data classification part1 - Kwon_sun_cheol&url=https://classicmania.github.io/posts/Imbalanced_classification_part1/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><h3 data-toc-skip class="pl-3 pt-2 mb-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Imbalanced_classification_part2/"><div class="card-body"> <span class="timeago small" > Jan 24 <i class="unloaded">2021-01-24T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Imbalanced data classification part2</h3><div class="text-muted small"><p> 지난 포스트에 이어서 고객의 이탈 유무와 LTV를 예측하는 태스크를 진행할 때 ‘불균형 데이터를 어떻게 다룰 것인가’에 관하여 숙고하였던 내용을 다루겠습니다. 불균형 데이터는 데이터 자체의 크기, 노이즈를 유발하는 데이터, 왜도가 높은 데이터 분포 문제와 관련이 깊습니다. 지난 포스트에서 우리는 명쾌한 클래스의 분류(Ex : 이탈 또는 비...</p></div></div></a></div><div class="card"> <a href="/posts/SparkTuningPart1/"><div class="card-body"> <span class="timeago small" > Feb 21 <i class="unloaded">2021-02-21T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Spark Tuning과 관련된 몇 가지 발견들</h3><div class="text-muted small"><p> 최근에 고객 세분화(Customer Segmentation) RFM 모델 디버깅 작업과 팀원 중 한분께서 Session Based RS를 고안하실 때 필요한 순차 데이터(Sequential Data) 전처리 작업을 스파크로 진행하면서 경험하였던 것들 중 몇 가지를 공유하고자 합니다. Spark Tuning 몇 가지 전제들 ‘Data...</p></div></div></a></div><div class="card"> <a href="/posts/Similarityandmetrics/"><div class="card-body"> <span class="timeago small" > Apr 18 <i class="unloaded">2021-04-18T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Recommendation system for E-commerce / Similarity and Metrics</h3><div class="text-muted small"><p> 지난 시간에는 팀원들과 Matrix Completion과 관련된 SVD1, ALS2, SGD의 개념을 살펴보았습니다. 이번 시간에는 이커머스에서 추천 모델의 최종 단계인 유사도(Similarity)를 계산하는 부분과 지표(Metric)를 점검해보았습니다. Similarity 상품 또는 고객을 벡터로 표현한 뒤 벡터 간의 유사...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/KernelDensity/" class="btn btn-outline-primary"><p>Kernel Density Estimation</p></a> <a href="/posts/Imbalanced_classification_part2/" class="btn btn-outline-primary"><p>Imbalanced data classification part2</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://github.com/classicmania">Kwon_suncheol</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/similarity/">Similarity</a> <a class="post-tag" href="/tags/recommendation/">Recommendation</a> <a class="post-tag" href="/tags/imbalanced_data/">Imbalanced_data</a> <a class="post-tag" href="/tags/evaluation_metric/">Evaluation_metric</a> <a class="post-tag" href="/tags/data_sampling/">Data_sampling</a> <a class="post-tag" href="/tags/classification/">Classification</a> <a class="post-tag" href="/tags/churn_prediction/">Churn_prediction</a> <a class="post-tag" href="/tags/stack/">stack</a> <a class="post-tag" href="/tags/queue/">queue</a> <a class="post-tag" href="/tags/linked_list/">linked_list</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://classicmania.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
